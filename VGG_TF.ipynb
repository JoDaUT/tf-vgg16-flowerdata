{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Solrak97/tf-vgg16-flowerdata/blob/main/VGG_TF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUk44wlBUSqO"
      },
      "source": [
        "# Transferencia de aprendizaje utilizando VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xj5_bnO3VsAq"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! rm -rf flowers\n",
        "! kaggle datasets download alxmamaev/flowers-recognition\n",
        "! unzip flowers-recognition.zip\n",
        "! rm -r flowers-recognition.zip\n",
        "! rm -rf flowers/sample_data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zf79yxuCRcO-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torchvision import datasets, transforms, models\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import Subset\n",
        "import pickle\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyOqFme5YBwM"
      },
      "source": [
        "## Carga del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eRkJOnpFSA7i"
      },
      "outputs": [],
      "source": [
        "transform_train = transforms.Compose([transforms.Resize((224,224)),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)),\n",
        "                                      transforms.ColorJitter(brightness=1, contrast=1, saturation=1),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                               ])\n",
        "\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize((224,224)),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                               ])\n",
        "\n",
        "dataset = ImageFolder(\"flowers\")\n",
        "train, val = torch.utils.data.random_split(dataset, [4317 - 863, 863], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "train.dataset.transform = transform_train\n",
        "val.dataset.transform = transform\n",
        "\n",
        "training_loader = torch.utils.data.DataLoader(train, batch_size=200, shuffle=True)\n",
        "validation_loader = torch.utils.data.DataLoader(val, batch_size = 200, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ip5eZ-6OSVsg"
      },
      "outputs": [],
      "source": [
        "def im_convert(tensor):\n",
        "  image = tensor.cpu().clone().detach().numpy()\n",
        "  image = image.transpose(1, 2, 0)\n",
        "  image = image * np.array((0.5, 0.5, 0.5)) + np.array((0.5, 0.5, 0.5))\n",
        "  image = image.clip(0, 1)\n",
        "  return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywZj89_xTyUk"
      },
      "source": [
        "## Modificación del modelo base VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4QtN8tcYSZCo"
      },
      "outputs": [],
      "source": [
        "classes = ('margaritas', 'rosas', 'dientes de león', 'tulipanes', 'girasoles')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQuEWoCyScV_",
        "outputId": "6f5cacc1-2555-4040-ad8e-df584e85bb67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# Creación del modelo VGG16\n",
        "model = models.vgg16(pretrained=True)\n",
        "\n",
        "# Congelamiento del modelo base\n",
        "for param in model.features.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "n_inputs = model.classifier[6].in_features\n",
        "last_layer = nn.Linear(n_inputs, len(classes))\n",
        "\n",
        "# Reemplzo de la capa de salida\n",
        "model.classifier[6] = last_layer\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW2OLHbEbKBG"
      },
      "source": [
        "## Entrenamiento del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5isVa-FPbLzX"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYHw0ViGbYsT",
        "outputId": "602b762a-d7cd-4e75-bdbb-5e69fd432e9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 1\n",
            "training loss: 0.0041, acc 0.6948 \n",
            "validation loss: 0.0025, validation acc 0.8494 \n"
          ]
        }
      ],
      "source": [
        "epochs = 10\n",
        "running_loss_history = []\n",
        "running_corrects_history = []\n",
        "val_running_loss_history = []\n",
        "val_running_corrects_history = []\n",
        "\n",
        "for e in range(epochs):\n",
        "  \n",
        "  running_loss = 0.0\n",
        "  running_corrects = 0.0\n",
        "  val_running_loss = 0.0\n",
        "  val_running_corrects = 0.0\n",
        "  \n",
        "  for inputs, labels in training_loader:\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    _, preds = torch.max(outputs, 1)\n",
        "    running_loss += loss.item()\n",
        "    running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "  else:\n",
        "    with torch.no_grad():\n",
        "      for val_inputs, val_labels in validation_loader:\n",
        "        val_inputs = val_inputs.to(device)\n",
        "        val_labels = val_labels.to(device)\n",
        "        val_outputs = model(val_inputs)\n",
        "        val_loss = criterion(val_outputs, val_labels)\n",
        "\n",
        "        _, val_preds = torch.max(val_outputs, 1)\n",
        "        val_running_loss += val_loss.item()\n",
        "        val_running_corrects += torch.sum(val_preds == val_labels.data)\n",
        "      \n",
        "    epoch_loss = running_loss/len(training_loader.dataset)\n",
        "    epoch_acc = running_corrects.float()/ len(training_loader.dataset)\n",
        "    running_loss_history.append(epoch_loss)\n",
        "    running_corrects_history.append(epoch_acc)\n",
        "    \n",
        "    val_epoch_loss = val_running_loss/len(validation_loader.dataset)\n",
        "    val_epoch_acc = val_running_corrects.float()/ len(validation_loader.dataset)\n",
        "    val_running_loss_history.append(val_epoch_loss)\n",
        "    val_running_corrects_history.append(val_epoch_acc)\n",
        "    print('epoch :', (e+1))\n",
        "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
        "    print('validation loss: {:.4f}, validation acc {:.4f} '.format(val_epoch_loss, val_epoch_acc.item()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Guardar modelo"
      ],
      "metadata": {
        "id": "DEh89BxZC0ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(model, path):\n",
        "    # Basic details\n",
        "    checkpoint = {\n",
        "        'class_to_idx': model.class_to_idx,\n",
        "        'idx_to_class': model.idx_to_class,\n",
        "        'epochs': model.epochs,\n",
        "    }\n",
        "\n",
        "    # Extract the final classifier and the state dictionary\n",
        "    \n",
        "    # Check to see if model was parallelized\n",
        "    if torch.cuda.is_available():\n",
        "      if torch.cuda.device_count() > 1:\n",
        "        checkpoint['classifier'] = model.module.classifier\n",
        "        checkpoint['state_dict'] = model.module.state_dict()\n",
        "\n",
        "    else:\n",
        "       checkpoint['classifier'] = model.classifier\n",
        "       checkpoint['state_dict'] = model.state_dict()\n",
        "\n",
        "    # Add the optimizer\n",
        "    checkpoint['optimizer'] = model.optimizer\n",
        "    checkpoint['optimizer_state_dict'] = model.optimizer.state_dict()\n",
        "\n",
        "    # Save the data to the path\n",
        "    torch.save(checkpoint, path)"
      ],
      "metadata": {
        "id": "abT40udzHcFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cargar modelo"
      ],
      "metadata": {
        "id": "tv6ijtpdHeSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(path, device):\n",
        "\n",
        "    # Load in checkpoint\n",
        "    checkpoint = torch.load(path)\n",
        "\n",
        "    \n",
        "    model = models.vgg16(pretrained=True)\n",
        "    # Make sure to set parameters as not trainable\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    model.classifier = checkpoint['classifier']\n",
        "\n",
        "\n",
        "    # Load in the state dict\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f'{total_params:,} total parameters.')\n",
        "    total_trainable_params = sum(\n",
        "        p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f'{total_trainable_params:,} total gradient parameters.')\n",
        "\n",
        "    # Move to device\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Model basics\n",
        "    model.class_to_idx = checkpoint['class_to_idx']\n",
        "    model.idx_to_class = checkpoint['idx_to_class']\n",
        "    model.epochs = checkpoint['epochs']\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = checkpoint['optimizer']\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    return model, optimizer"
      ],
      "metadata": {
        "id": "GS3o_N-RHfwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_checkpoint(model, 'classifier.trch')\n",
        "load_checkpoint('classifier.trch', device)"
      ],
      "metadata": {
        "id": "tlbJgcIUC25T"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "VGG-TF.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMktHGO1JIkv2wcuokRNkYT",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}